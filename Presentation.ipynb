{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classificaton using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Set Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_requests = pd.read_excel('frc_data.xlsx', sheet_name='frc data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_requests['Clean Description'] = df_requests['Request Description'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condense LOB Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "monoline_list = ['Equipment Breakdown', 'Inland Marine', 'International', 'Ocean Marine']\n",
    "package_general_liability_list = ['Connect CNP', 'General Liability', 'Multiline']\n",
    "package_property_list = ['Paramount Package', 'Property']\n",
    "condensed_lob_list = []\n",
    "for row in df_requests['LOB']:\n",
    "    if row in monoline_list:\n",
    "        condensed_lob_list.append('Monoline')\n",
    "    elif row in package_general_liability_list:\n",
    "        condensed_lob_list.append('Package General Liability')\n",
    "    elif row in package_property_list:\n",
    "        condensed_lob_list.append('Package Property')\n",
    "    else:\n",
    "        condensed_lob_list.append(row)\n",
    "        \n",
    "df_requests['Updated LOB'] = condensed_lob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Package General Liability    2407\n",
       "Auto                         1948\n",
       "Workers Comp                 1665\n",
       "Package Property             1438\n",
       "Umbrella                      878\n",
       "Not LOB Specific              561\n",
       "Monoline                      416\n",
       "Name: Updated LOB, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_requests['Updated LOB'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word_list = ['Rate Change',\n",
    "'Form Review',\n",
    "'Endorsement Print',\n",
    "'Premium Discrepancy',\n",
    "'CAT',\n",
    "'Endorsement Process',\n",
    "'Experience Modification',\n",
    "'Billing',\n",
    "'System Issue']\n",
    "condensed_list = []\n",
    "for row in df_requests['Key Word Roll-Up']:\n",
    "    if row in key_word_list:\n",
    "        condensed_list.append(row)\n",
    "    else: \n",
    "        condensed_list.append('Other')\n",
    "        \n",
    "df_requests['key_words'] = condensed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "from pylab import rcParams\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9313, 14307)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "sb.set_style('whitegrid')\n",
    "df_requests = pd.read_excel('frc_data.xlsx', sheet_name='frc data')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_requests['Request Description'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6984, 14307)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = pd.get_dummies(df_requests['Action Needed'],drop_first=True)\n",
    "action.Y.value_counts()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, action.Y, random_state=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1680,  266],\n",
       "       [ 163,  220]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test, y_test)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8158007728638901"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1843, 1: 486})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2329"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1843 + 486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7913267496779733"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1843 / 2329"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Text Classification - LOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6705 samples, validate on 745 samples\n",
      "Epoch 1/6\n",
      "6705/6705 [==============================] - 9s 1ms/step - loss: 1.4985 - acc: 0.4752 - val_loss: 1.3032 - val_acc: 0.5477\n",
      "Epoch 2/6\n",
      "6705/6705 [==============================] - 8s 1ms/step - loss: 0.9301 - acc: 0.6904 - val_loss: 1.1655 - val_acc: 0.5973\n",
      "Epoch 3/6\n",
      "6705/6705 [==============================] - 8s 1ms/step - loss: 0.7183 - acc: 0.7614 - val_loss: 1.1990 - val_acc: 0.5946\n",
      "Epoch 4/6\n",
      "6705/6705 [==============================] - 8s 1ms/step - loss: 0.5859 - acc: 0.8098 - val_loss: 1.1992 - val_acc: 0.5987\n",
      "Epoch 5/6\n",
      "6705/6705 [==============================] - 8s 1ms/step - loss: 0.4849 - acc: 0.8510 - val_loss: 1.2446 - val_acc: 0.5852\n",
      "Epoch 6/6\n",
      "6705/6705 [==============================] - 8s 1ms/step - loss: 0.4014 - acc: 0.8868 - val_loss: 1.3020 - val_acc: 0.5852\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(df_requests) * .8)\n",
    "train_posts = df_requests['Clean Description'][:train_size]\n",
    "train_tags = df_requests['Updated LOB'][:train_size]\n",
    "test_posts = df_requests['Clean Description'][train_size:]\n",
    "test_tags = df_requests['Updated LOB'][train_size:]\n",
    "\n",
    "vocab_size = 1000\n",
    "tokenize = text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=100, \n",
    "                    epochs=6, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1522,    2,    7,   47,   20,    6,   26],\n",
       "       [   8,  246,    3,   19,   15,    3,    3],\n",
       "       [  23,    3,  400,   66,   24,    9,   16],\n",
       "       [  41,    2,   15, 1654,   72,   15,   36],\n",
       "       [  29,    4,    7,   61, 1063,    4,    7],\n",
       "       [  18,    2,    4,   36,   10,  566,   13],\n",
       "       [  22,    2,    5,   32,   15,   13, 1234]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_train)\n",
    "np.argmax(predictions[0])\n",
    "predicted_labels = [np.argmax(x) for x in predictions]\n",
    "y_labels = [np.argmax(x) for x in y_train]\n",
    "confusion_matrix(y_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Count Vectorizer N Grams - LOB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ZGS/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9313, 163488)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=sw,ngram_range=(1,2))\n",
    "train_posts = vectorizer.fit_transform(df_requests['Clean Description'].values.astype('U'))\n",
    "train_posts.shape\n",
    "#x = v.fit_transform(df['Review'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9313,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(df_requests) * .8)\n",
    "train_posts = df_requests['Clean Description']#[:train_size]\n",
    "train_tags = df_requests['Updated LOB']#[:train_size]\n",
    "test_posts = df_requests['Clean Description']#[train_size:]\n",
    "test_tags = df_requests['Updated LOB']#[train_size:]\n",
    "train_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Train on 8381 samples, validate on 932 samples\n",
      "Epoch 1/4\n",
      "8381/8381 [==============================] - 21s 2ms/step - loss: 1.4508 - acc: 0.5133 - val_loss: 1.1497 - val_acc: 0.6180\n",
      "Epoch 2/4\n",
      "8381/8381 [==============================] - 20s 2ms/step - loss: 0.7932 - acc: 0.7494 - val_loss: 1.0531 - val_acc: 0.6245\n",
      "Epoch 3/4\n",
      "8381/8381 [==============================] - 20s 2ms/step - loss: 0.5316 - acc: 0.8355 - val_loss: 1.0853 - val_acc: 0.6234\n",
      "Epoch 4/4\n",
      "8381/8381 [==============================] - 19s 2ms/step - loss: 0.3861 - acc: 0.8876 - val_loss: 1.0842 - val_acc: 0.6170\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 4000\n",
    "tokenize = text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "#print(x_train.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "#y_train.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(x_train.shape[1],)))\n",
    "#model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=100, \n",
    "                    epochs=4, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook FastText Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__label__Auto Today I watched the Commercial Auto EPC Training Webinar 5, Other Transactions/Auto Tips & Tricks.  The trainer mentioned that the NBRI document has been updated with the separation of the Comprehensive and Collision losses which I can see.When setting up an account today for new business rating (effective 08/01/18), I noticed that the Loss Summary tab on the NBRI doesnot require the following information:1.  Separate entry of lossexpenses2.  Each loss date3.  Carrier Name4.  Policy NumberAre these requirements obsolete for EPC Automobile Experience Rating?  Please advise.  Thanks.\n",
    "__label__Package-General-Liability Do we have a version of the sexual abuse and molestation questionnaire that is not catered to education risks? I have a copy of the EDU version but don't want to confuse my insured.Thanks,\n",
    "__label__Package-General-Liability ​On a GL Composite Rated policy (Paramount Package), the user added forms CG 20 37 and CNA-74745 which was supposed to generate a 5% premium but did not.  The policy was already composite rated so will he have to remove the composite rate then add the form?\n",
    "__label__Workers-Comp ​Please review this one, why did we add the officers back on? Are their errors to correct?\n",
    "__label__Workers-Comp ​RE factors not available for WC policies - due to Prior Carrier data issue.  Please report Ticket to Help Desk.  Can you look into this, please?  Thanks. \n",
    "__label__Not-LOB-Specific ​how do I update my UW remarks?\n",
    "__label__Auto ​I've converted policy once this am with one composite rate regardless of state or type.  Rated fine – no errors UW has ask that I composite rate per vehicle type and all the states carry the same rate (per type).I've created and assigned groups but now I'm getting the following error:  Aug 21, 20188:58 AMErrors during initial rate of 6072603997 - Submission​The following errors were found when rating the 6072603997 - Submission:Policy has a composite rate that does not allow Michigan to rate both PIP and coverage. Evaluate the composite rate or create a group composite for all states separating Michigan (EBR209171).​ Any suggestions?\n",
    "__label__Package-General-Liability ​Please expedite so we can quote.\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ clear\n",
    "\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ wc combined_text.txt\n",
    "    9313  513357 3109561 combined_text.txt\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ head -7313 combined_text.txt > data.train\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ tail -2000 combined_text.txt > data.valid\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ ./fasttext supervised -input data.train -output model_project -lr 1.0 -epoch 50\n",
    "Read 0M words\n",
    "Number of words:  38584\n",
    "Number of labels: 7\n",
    "Progress: 100.0%  words/sec/thread: 2776697  lr: 0.000000  loss: 0.179360  eta: 0h0m -14m \n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ ./fasttext test model_project.bin data.valid \n",
    "N\t2000\n",
    "P@1\t0.585\n",
    "R@1\t0.585\n",
    "Number of examples: 2000\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ ./fasttext supervised -input data.train -output model_project -lr 1.0 -epoch 50 -wordNgrams 2\n",
    "Read 0M words\n",
    "Number of words:  38584\n",
    "Number of labels: 7\n",
    "Progress: 100.0%  words/sec/thread: 1403889  lr: 0.000000  loss: 0.062241  eta: 0h0m \n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ ./fasttext test model_project.bin data.valid \n",
    "N\t2000\n",
    "P@1\t0.577\n",
    "R@1\t0.577\n",
    "Number of examples: 2000\n",
    "Zacharys-iMac:fastText-0.1.0 ZGS$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Text Classification - Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(df_requests) * .8)\n",
    "train_posts = df_requests['Clean Description'][:train_size]\n",
    "train_tags = df_requests['key_words'][:train_size]\n",
    "test_posts = df_requests['Clean Description'][train_size:]\n",
    "test_tags = df_requests['key_words'][train_size:]\n",
    "\n",
    "vocab_size = 1000\n",
    "tokenize = text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=100, \n",
    "                    epochs=6, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='.*/IPython/.*')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='pyLDAvis')\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#df_re = pd.read_excel('frc_data.xlsx', sheet_name='frc data')\n",
    "\n",
    "text = df_requests['Clean Description'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=max_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el215611125812072888455132326\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el215611125812072888455132326_data = {\"mdsDat\": {\"x\": [-0.11705134829066156, 0.2059821576638044, -0.11131732927498357, 0.08786996288515608, -0.2059718251079043, 0.14048838212458917], \"y\": [0.18285490484919328, -0.17243592201268163, -0.038095626231939475, 0.08456463441622875, -0.15367664880899837, 0.09678865778819719], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [20.684471580741288, 16.79972574033496, 16.278698640941585, 15.722306246895535, 15.510226673225148, 15.004571117861468]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [1686.0, 1530.0, 1791.0, 2080.0, 4811.0, 1418.0, 975.0, 942.0, 1382.0, 672.0, 825.0, 1680.0, 1387.0, 1312.0, 1131.0, 1482.0, 474.0, 908.0, 721.0, 1458.0, 207.11899236595428, 225.88117007385176, 148.28979392395547, 147.00801889825118, 173.36500456071624, 146.69785467709787, 104.6962423216433, 129.37826809333635, 115.05275049038272, 127.5296554784651, 79.54230150254212, 74.82064082502477, 70.61774241910506, 69.02188363934653, 68.63684893346299, 60.13623229216359, 68.13256510183709, 63.55072872535034, 54.17552987029257, 54.6431900859552, 251.59799697692569, 146.96722404787437, 200.26974053709088, 90.5144265209401, 234.28420476410358, 177.657346077632, 620.264556643899, 193.0400196955761, 130.03023594665132, 326.6439854727577, 383.2875607756466, 502.3094569383705, 232.4972002071338, 496.69125901887753, 474.2266681244908, 366.4181274719121, 370.8955379582095, 717.6948146817946, 581.2656100048357, 587.6524526422501, 478.73887628182536, 481.2130825732661, 274.19710951295383, 459.87670456687243, 456.492703847129, 377.5017507409446, 276.79002761083416, 278.912585196259, 300.50100892161277, 281.64055881635596, 277.17603020092497, 197.10081339599358, 173.71953562664106, 161.67460525384638, 289.3169054269852, 138.76857730938974, 125.7268692041798, 124.17590267911577, 71.99015797356951, 78.12247240539926, 66.03850035248638, 63.11863317470113, 117.28336118043575, 57.51339329027799, 55.570696429775, 424.9279006297462, 46.329750187833874, 178.2975920318589, 46.696514972818036, 46.59780232388172, 141.55988428109882, 312.6758156909093, 148.54230348828614, 67.39618133630648, 1599.7951749965268, 474.39865346455235, 121.22774754741722, 1583.703886581331, 315.3011515081665, 189.6987254826608, 536.9829963920726, 709.0355352717958, 656.7377288252769, 507.37391878406004, 265.90300386352106, 627.2669252859303, 248.78041326451705, 237.379346663481, 480.4880246485926, 475.5544796176224, 342.37836858128577, 301.54937199235025, 294.60818561344297, 360.1550211691566, 275.81325701089196, 1684.17344482974, 297.43602381022214, 213.7070744229139, 204.93922491233897, 203.7359520305539, 171.41144689157775, 118.03127631692952, 120.82802713713235, 111.7227273421245, 100.3751333306596, 95.32647181786571, 95.84617624334787, 81.41249234833448, 80.31560730349798, 73.83140351029529, 71.38400320052952, 70.62853073247123, 85.83997522170594, 118.09681671697795, 53.80175057673807, 134.44277511341946, 388.5871137414669, 397.05462638218404, 241.94598053689415, 324.2513145548117, 111.24438405834586, 451.7197607822289, 246.16446106722498, 535.1333580451472, 321.4994648095728, 829.0155536243949, 805.6356990474494, 634.4034485610085, 1272.1919415873317, 229.51773782926242, 365.7843224491371, 402.63177917851556, 450.7688589573111, 307.59413958302855, 331.3545898151721, 406.2908280862567, 301.15281768666125, 293.79691977356794, 235.66421132388254, 219.78704540602808, 137.51025532014748, 138.68237860852557, 73.0823533517681, 141.0038171403863, 63.74282382890577, 136.2918481898911, 54.47523420152496, 47.996711312501766, 44.22578826684106, 37.24936390993621, 179.39072146597823, 59.83685036926591, 153.72370440822442, 32.01299878063499, 62.276593639783464, 66.68679693001503, 245.01996503420463, 845.0426600550222, 677.2462432336731, 277.87884749968873, 307.72326573743055, 714.6389608723861, 212.2873971929882, 935.8788155175746, 518.1764771704426, 272.08633829933683, 140.2659477411536, 337.09172954116326, 202.16200728292105, 555.1527213877557, 1210.9571609845527, 579.9198787617145, 355.5585149300265, 459.4306864080921, 264.7205451849308, 272.4553703939045, 311.11434872953987, 277.6385602163036, 248.4442456941153, 245.80255548341052, 204.92339088896583, 199.561555096182, 167.38979541523466, 152.58851857855004, 138.73395131019436, 1518.3768228811127, 116.82162404608292, 98.6337244080518, 102.9599231793624, 85.41722526441725, 84.86093619352404, 79.46777874002558, 118.1294750745695, 77.24798634716772, 69.41499448008848, 98.58546035681226, 83.02090405362283, 63.17007241014726, 68.32488204500028, 71.91401612890283, 439.9372518465031, 180.72355422333925, 195.39215675576676, 217.09234685321678, 147.22278044381488, 116.89027093599684, 224.40663062066022, 130.1042276321062, 99.7244856986614, 917.3772694915818, 529.2531217023852, 261.1470826098923, 401.53821318008426, 209.24940431430315, 178.72700396993957, 492.43536492672354, 155.17777234130483, 446.2412093462799, 650.4716948398913, 395.723024864683, 281.41992303847707, 196.76910796659143, 237.23621602426425, 249.9385068227027, 249.59795447253717, 237.3922955173036, 232.061153620017, 473.4639784835016, 151.1601280554469, 104.55834508536213, 290.0262191530613, 81.60254029718459, 80.32513815576549, 73.26280521319647, 64.01296463777999, 59.72776047430943, 64.02951560778712, 67.65198290666385, 56.03116674339256, 47.78619449655123, 84.340486492893, 48.2020433727458, 54.52249596209474, 44.06187451880028, 194.18271051769855, 117.17195276893963, 37.71331299672953, 652.5781834927296, 80.44773111829785, 431.01762741205664, 843.549972946182, 196.49359239752403, 374.3118969351886, 237.70507065880386, 366.8224377981713, 287.40138001097586, 195.15636575128232, 262.1516187794458, 623.466865398615, 238.56889107196687, 190.85034315422845, 733.5245772702032, 1426.7336641424265, 566.0348127291024, 282.89639162197085, 220.68590257351858, 332.5801630334189, 477.40919444894564, 380.7318668938058, 356.1833985915433, 300.6372716431145, 297.08802386097034, 267.868876086894], \"Term\": [\"endorsement\", \"form\", \"rate\", \"premium\", \"policy\", \"coverage\", \"gl\", \"18\", \"insured\", \"2018\", \"error\", \"help\", \"rst\", \"renewal\", \"issue\", \"agent\", \"loss\", \"rating\", \"state\", \"auto\", \"cat\", \"dmf\", \"model\", \"documents\", \"right\", \"submission\", \"rapid\", \"tool\", \"direction\", \"run\", \"modeling\", \"excel\", \"zone\", \"stop\", \"save\", \"uts\", \"feedback\", \"guide\", \"customer\", \"eb\", \"access\", \"com\", \"underwriting\", \"emails\", \"accounts\", \"forward\", \"cna\", \"open\", \"application\", \"send\", \"email\", \"frc\", \"document\", \"team\", \"good\", \"morning\", \"able\", \"account\", \"hi\", \"thanks\", \"new\", \"thank\", \"process\", \"agent\", \"help\", \"need\", \"business\", \"information\", \"epc\", \"know\", \"hello\", \"fast\", \"damage\", \"option\", \"increase\", \"physical\", \"applied\", \"eligible\", \"vs\", \"collision\", \"mods\", \"debit\", \"higher\", \"trailers\", \"deductibles\", \"000\", \"lower\", \"total\", \"discretionary\", \"40\", \"calculation\", \"mod\", \"decrease\", \"credits\", \"rate\", \"expiring\", \"increased\", \"premium\", \"class\", \"track\", \"year\", \"change\", \"showing\", \"property\", \"limit\", \"auto\", \"schedule\", \"exposure\", \"renewal\", \"rst\", \"rating\", \"correct\", \"quote\", \"account\", \"tap\", \"endorsement\", \"ny\", \"ncci\", \"proof\", \"endt\", \"fein\", \"bureau\", \"department\", \"ap\", \"registered\", \"linked\", \"workers\", \"inception\", \"inspection\", \"rp\", \"entities\", \"compensation\", \"electronic\", \"mo\", \"crit\", \"entity\", \"endorsements\", \"processed\", \"audit\", \"named\", \"filing\", \"location\", \"payroll\", \"state\", \"states\", \"insured\", \"agent\", \"attached\", \"policy\", \"address\", \"number\", \"wc\", \"advise\", \"added\", \"request\", \"coverage\", \"hello\", \"hi\", \"cue\", \"technical\", \"middle\", \"market\", \"cf\", \"small\", \"mm\", \"commission\", \"occurred\", \"tria\", \"approve\", \"bind\", \"desk\", \"progress\", \"asap\", \"blocking\", \"underlying\", \"wait\", \"ies\", \"gl\", \"error\", \"center\", \"field\", \"issue\", \"message\", \"help\", \"rating\", \"getting\", \"ticket\", \"let\", \"contact\", \"rst\", \"policy\", \"need\", \"quote\", \"know\", \"umbrella\", \"resource\", \"account\", \"thanks\", \"premium\", \"renewal\", \"mike\", \"blanket\", \"binder\", \"wording\", \"aid\", \"form\", \"contractors\", \"ba\", \"contractor\", \"manuscript\", \"noc\", \"marine\", \"waiver\", \"ai\", \"approval\", \"product\", \"uim\", \"inland\", \"trucks\", \"excluded\", \"forms\", \"um\", \"job\", \"exclusion\", \"covered\", \"symbol\", \"notice\", \"approved\", \"plus\", \"coverage\", \"add\", \"use\", \"question\", \"non\", \"ca\", \"insured\", \"hired\", \"auto\", \"policy\", \"need\", \"does\", \"liability\", \"provide\", \"know\", \"thanks\", \"hi\", \"attached\", \"loss\", \"losses\", \"2019\", \"01\", \"05\", \"history\", \"runs\", \"edit\", \"loc\", \"engine\", \"07\", \"pm\", \"expiration\", \"mvr\", \"09\", \"08\", \"copies\", \"years\", \"status\", \"utilized\", \"2018\", \"06\", \"17\", \"18\", \"16\", \"date\", \"12\", \"effective\", \"2017\", \"report\", \"19\", \"term\", \"data\", \"bua\", \"epc\", \"policy\", \"renewal\", \"10\", \"prior\", \"number\", \"account\", \"new\", \"rst\", \"policies\", \"wc\", \"auto\"], \"Total\": [1686.0, 1530.0, 1791.0, 2080.0, 4811.0, 1418.0, 975.0, 942.0, 1382.0, 672.0, 825.0, 1680.0, 1387.0, 1312.0, 1131.0, 1482.0, 474.0, 908.0, 721.0, 1458.0, 207.96612916518586, 226.82524734543932, 149.137955795817, 147.8578550966232, 174.4403127470364, 147.62345036199227, 105.56999227012365, 130.4634311566734, 116.05774897504283, 128.6588617975824, 80.3908633897927, 75.67258023731138, 71.48083109200446, 69.87668132040817, 69.62638832493946, 61.02461751905554, 69.14969166433929, 64.55000408424807, 55.04049731505894, 55.539422499695306, 262.73403544690535, 152.41040890535584, 212.54059670560542, 93.08877528757128, 253.0142481360154, 190.65328182573597, 727.3238464025843, 218.90534392799043, 141.4570308469656, 440.9085747223066, 546.8068006563963, 769.3362168059097, 298.6654050629729, 867.6539730448196, 821.6147678757374, 585.2214503145346, 608.1649628603014, 1906.2531168036048, 1448.5612163214903, 1588.5688421007999, 1176.3513257304996, 1257.4380053587893, 469.3741417078339, 1482.6178956797935, 1680.3100673110018, 1813.7066580627545, 653.4952011906569, 778.6034027429723, 1385.6007472395484, 1321.8787025816907, 994.2361813113944, 198.03522854177893, 174.5658034078095, 162.53966118932462, 291.02226737078365, 139.62627010704333, 126.57491802148826, 125.02364598918511, 72.84484670345573, 79.08799142986987, 66.88417333213714, 63.96205723805964, 118.86404825929043, 58.362561781005674, 56.41869927851098, 432.4079799753004, 47.25337010296601, 181.86356356394194, 47.6447277415216, 47.570059678418524, 144.83941525896574, 321.5048229839627, 153.98140094810933, 69.07533699433854, 1791.3313685332066, 515.8512075642319, 127.74055308906527, 2080.6115714271973, 374.59177230477417, 224.18994127975688, 762.1933953288143, 1107.617950675205, 1013.2315433811438, 804.4924042622474, 356.0218475056501, 1458.3004220927646, 361.19795883640535, 336.3080235062421, 1312.0952286265488, 1387.6012473893925, 908.3529987220994, 841.9734238522532, 803.6951574089633, 1906.2531168036048, 675.0020032273284, 1686.4050357770464, 298.3029224061377, 214.55624205377583, 205.77644109063203, 204.57920044434428, 172.24649133994237, 118.86606922045989, 121.71717029172133, 112.57061767957994, 101.21077827513761, 96.16339296376785, 96.69359330220095, 82.25223113516671, 81.15053021945913, 74.67928127460108, 72.23074116683252, 71.46666271119868, 86.88074673687615, 119.72990736574293, 54.63900289169046, 137.27131992433763, 425.3050306586624, 439.5493556804748, 267.328259393243, 376.7571667135303, 116.49591202302264, 568.2425801027937, 286.7661095735734, 721.7941869166228, 424.6650878738224, 1382.8377133376937, 1482.6178956797935, 1240.5521005996384, 4811.563156326525, 333.08577202511435, 783.1139319758372, 1011.2504782654929, 1270.075820799299, 594.7704436437305, 788.8507930434641, 1418.952978534161, 994.2361813113944, 1448.5612163214903, 236.57390553714126, 220.63940505092054, 138.37920917911956, 139.6944131078419, 73.93793917133726, 142.81576575198847, 64.60182977915092, 138.14460625698607, 55.359130849611574, 48.857279661279144, 45.5470446568291, 38.564264547659896, 186.17202833071167, 62.51340587194133, 172.03519086505517, 36.14138785882384, 70.38683866026057, 75.82082783044174, 281.4600650786295, 975.6576057985117, 825.5201179069581, 344.6139282039342, 416.8677943181061, 1131.482556830803, 286.4287837960404, 1680.3100673110018, 908.3529987220994, 410.34434419780655, 185.9436769396841, 640.8015799024568, 313.2057567837907, 1387.6012473893925, 4811.563156326525, 1813.7066580627545, 803.6951574089633, 1321.8787025816907, 525.4796622659763, 580.8530847933448, 1906.2531168036048, 1588.5688421007999, 2080.6115714271973, 1312.0952286265488, 205.77420446998607, 200.41141804032506, 168.24031428638924, 153.4323281626234, 139.64848642109345, 1530.4842074124588, 117.78436556447537, 99.47851740394195, 103.97111989890335, 86.27042881175123, 85.71243567514796, 80.31699595814665, 119.3994549187441, 78.13449967286009, 70.26794730351175, 99.87796670353313, 84.11141519503316, 64.0166301875129, 69.24060798283188, 72.8884900631278, 446.5995510005848, 183.72421856032838, 202.43323763819362, 226.16398817122604, 155.92905544973235, 121.9895213980248, 249.16428555099918, 138.06935656732875, 103.1695891209044, 1418.952978534161, 809.6790669986373, 399.13305007563633, 744.775895097196, 292.23496221171786, 239.65850668902064, 1382.8377133376937, 190.454827336146, 1458.3004220927646, 4811.563156326525, 1813.7066580627545, 803.392497067959, 323.25895407211203, 628.8513072909118, 1321.8787025816907, 1588.5688421007999, 1448.5612163214903, 1240.5521005996384, 474.7183280962182, 152.00513388010629, 105.41913225936784, 292.7374702999009, 82.45445051037765, 81.1722089266606, 74.12886057827382, 64.86233289195962, 60.57867935043089, 65.02419797757457, 68.7911084016629, 57.0163494946402, 48.63101666248846, 85.84686786268637, 49.143102238678075, 55.64673074837845, 44.975040654185605, 198.25575681648277, 119.66133772720919, 38.568615222364116, 672.6640418061501, 82.57856554824212, 463.86684715356273, 942.8034315717304, 219.5236054120266, 454.7505826519164, 286.75273152631405, 472.19845195900143, 366.0665773041583, 237.15175760567416, 344.0757319778905, 985.7376244859744, 318.69277614148126, 243.55263868189627, 1385.6007472395484, 4811.563156326525, 1312.0952286265488, 504.01406007207095, 328.0448161072196, 783.1139319758372, 1906.2531168036048, 1176.3513257304996, 1387.6012473893925, 857.3125950144804, 1011.2504782654929, 1458.3004220927646], \"loglift\": [20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5717, 1.5716, 1.5701, 1.57, 1.5696, 1.5695, 1.5675, 1.5674, 1.5671, 1.567, 1.5652, 1.5645, 1.5636, 1.5635, 1.5615, 1.5611, 1.561, 1.5602, 1.5599, 1.5595, 1.5325, 1.5394, 1.5163, 1.5477, 1.4989, 1.5052, 1.4166, 1.45, 1.4916, 1.2758, 1.2205, 1.1495, 1.3253, 1.018, 1.0262, 1.1076, 1.0813, 0.5989, 0.6627, 0.5813, 0.6768, 0.6153, 1.0382, 0.4052, 0.2726, 0.0062, 0.7167, 0.5492, 0.0473, 0.0296, 0.2985, 1.7791, 1.7789, 1.7785, 1.7779, 1.7776, 1.7771, 1.777, 1.772, 1.7715, 1.7711, 1.7705, 1.7704, 1.7692, 1.7687, 1.7664, 1.7641, 1.764, 1.7637, 1.7632, 1.7609, 1.756, 1.7478, 1.7592, 1.6707, 1.7, 1.7315, 1.5109, 1.6115, 1.6168, 1.4336, 1.3377, 1.3502, 1.3228, 1.4919, 0.9402, 1.411, 1.4354, 0.7792, 0.713, 0.8081, 0.757, 0.7802, 0.1174, 0.8888, 1.814, 1.8124, 1.8113, 1.8112, 1.8112, 1.8105, 1.8083, 1.808, 1.8078, 1.807, 1.8066, 1.8065, 1.8051, 1.805, 1.8039, 1.8035, 1.8035, 1.8033, 1.8016, 1.7999, 1.7945, 1.725, 1.7136, 1.7155, 1.6652, 1.7692, 1.5858, 1.6626, 1.5161, 1.537, 1.3037, 1.2054, 1.1447, 0.485, 1.4429, 1.0541, 0.8944, 0.7794, 1.1559, 0.9479, 0.5647, 0.621, 0.2199, 1.8462, 1.8462, 1.8438, 1.8428, 1.8385, 1.8373, 1.8367, 1.8366, 1.834, 1.8323, 1.8207, 1.8154, 1.813, 1.8063, 1.7375, 1.7288, 1.7277, 1.7217, 1.7114, 1.7064, 1.6521, 1.6349, 1.5465, 1.3906, 1.5505, 1.2648, 1.2888, 1.4392, 1.5682, 1.2077, 1.4123, 0.934, 0.4705, 0.7099, 1.0346, 0.7933, 1.1645, 1.0931, 0.0374, 0.1058, -0.2751, 0.1752, 1.8595, 1.8594, 1.8586, 1.8582, 1.8571, 1.8557, 1.8555, 1.8551, 1.8539, 1.8537, 1.8537, 1.853, 1.853, 1.8523, 1.8515, 1.8506, 1.8506, 1.8504, 1.8504, 1.8502, 1.8486, 1.8472, 1.8283, 1.8227, 1.8062, 1.821, 1.759, 1.8043, 1.8297, 1.4275, 1.4385, 1.4395, 1.2459, 1.5296, 1.5703, 0.8311, 1.6588, 0.6795, -0.1374, 0.3413, 0.8147, 1.3672, 0.8888, 0.1981, 0.0129, 0.0551, 0.1874, 1.8942, 1.8912, 1.8886, 1.8875, 1.8864, 1.8863, 1.8851, 1.8836, 1.8827, 1.8814, 1.8801, 1.8794, 1.8793, 1.8791, 1.8775, 1.8764, 1.8763, 1.8761, 1.8758, 1.8744, 1.8665, 1.8707, 1.8234, 1.7856, 1.786, 1.7022, 1.7092, 1.6443, 1.6549, 1.7019, 1.6249, 1.4387, 1.6072, 1.653, 1.2608, 0.6812, 1.0561, 1.3193, 1.5004, 1.0404, 0.5123, 0.7687, 0.5369, 0.8489, 0.6719, 0.2023], \"logprob\": [20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.2346, -5.1479, -5.5687, -5.5774, -5.4125, -5.5795, -5.9168, -5.7051, -5.8225, -5.7195, -6.1916, -6.2528, -6.3106, -6.3334, -6.339, -6.4712, -6.3464, -6.416, -6.5756, -6.567, -5.04, -5.5777, -5.2682, -6.0624, -5.1113, -5.388, -4.1377, -5.305, -5.7001, -4.779, -4.6191, -4.3486, -5.119, -4.3599, -4.4062, -4.6641, -4.6519, -3.9918, -4.2027, -4.1917, -4.3967, -4.3916, -4.954, -4.4369, -4.4443, -4.6343, -4.9446, -4.937, -4.8624, -4.9272, -4.9432, -5.0761, -5.2024, -5.2743, -4.6923, -5.427, -5.5257, -5.5381, -6.0833, -6.0016, -6.1696, -6.2148, -5.5952, -6.3078, -6.3422, -4.3079, -6.5241, -5.1764, -6.5162, -6.5183, -5.4071, -4.6147, -5.359, -6.1493, -2.9822, -4.1978, -5.5622, -2.9923, -4.6063, -5.1144, -4.0739, -3.7959, -3.8726, -4.1306, -4.7767, -3.9185, -4.8433, -4.8902, -4.185, -4.1954, -4.5239, -4.6509, -4.6742, -4.4733, -4.7401, -2.8993, -4.6331, -4.9637, -5.0056, -5.0115, -5.1843, -5.5574, -5.534, -5.6123, -5.7194, -5.771, -5.7656, -5.9288, -5.9424, -6.0266, -6.0603, -6.0709, -5.8759, -5.5568, -6.343, -5.4272, -4.3658, -4.3443, -4.8396, -4.5468, -5.6166, -4.2153, -4.8223, -4.0458, -4.5553, -3.6081, -3.6367, -3.8757, -3.1798, -4.8924, -4.4263, -4.3303, -4.2174, -4.5996, -4.5251, -4.3213, -4.6207, -4.6454, -4.8312, -4.9009, -5.3699, -5.3614, -6.002, -5.3448, -6.1387, -5.3788, -6.2958, -6.4224, -6.5043, -6.6759, -5.104, -6.2019, -5.2584, -6.8274, -6.162, -6.0936, -4.7922, -3.5542, -3.7755, -4.6664, -4.5644, -3.7218, -4.9356, -3.4521, -4.0432, -4.6874, -5.35, -4.4732, -4.9845, -3.9743, -3.1944, -3.9307, -4.4199, -4.1636, -4.7149, -4.6861, -4.5534, -4.6672, -4.7783, -4.789, -4.9573, -4.9839, -5.1597, -5.2522, -5.3474, -2.9546, -5.5193, -5.6886, -5.6456, -5.8324, -5.839, -5.9046, -5.5082, -5.933, -6.0399, -5.6891, -5.8609, -6.1341, -6.0557, -6.0045, -4.1933, -5.083, -5.005, -4.8997, -5.288, -5.5187, -4.8665, -5.4116, -5.6776, -3.4585, -4.0085, -4.7149, -4.2847, -4.9365, -5.0941, -4.0806, -5.2354, -4.1791, -3.8023, -4.2993, -4.6401, -4.9979, -4.8109, -4.7588, -4.7601, -4.8103, -4.833, -4.0868, -5.2285, -5.5971, -4.5769, -5.845, -5.8608, -5.9528, -6.0877, -6.157, -6.0875, -6.0325, -6.2209, -6.3801, -5.812, -6.3714, -6.2482, -6.4612, -4.978, -5.4832, -6.6168, -3.7659, -5.8592, -4.1807, -3.5092, -4.9662, -4.3217, -4.7758, -4.342, -4.586, -4.973, -4.6779, -3.8115, -4.7722, -4.9953, -3.649, -2.9837, -3.9082, -4.6018, -4.8501, -4.44, -4.0785, -4.3047, -4.3714, -4.5409, -4.5528, -4.6563]}, \"token.table\": {\"Topic\": [2, 4, 5, 6, 3, 5, 6, 6, 2, 3, 6, 6, 6, 6, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 5, 6, 2, 3, 4, 6, 2, 3, 4, 6, 2, 4, 6, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 6, 2, 1, 2, 3, 4, 5, 6, 1, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 5, 3, 1, 6, 2, 5, 4, 6, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 3, 6, 1, 2, 4, 5, 6, 5, 4, 5, 5, 5, 4, 6, 2, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 2, 3, 5, 6, 2, 4, 1, 1, 4, 6, 4, 2, 3, 4, 6, 2, 3, 4, 1, 3, 4, 6, 2, 1, 3, 6, 4, 5, 3, 1, 3, 4, 5, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 1, 2, 5, 2, 3, 3, 4, 1, 2, 1, 4, 6, 3, 5, 6, 2, 2, 4, 2, 3, 3, 4, 1, 2, 1, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 6, 3, 5, 6, 3, 2, 1, 2, 3, 4, 5, 6, 1, 3, 4, 3, 4, 1, 2, 3, 4, 5, 3, 6, 3, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 6, 1, 5, 3, 5, 6, 2, 4, 6, 1, 2, 3, 4, 2, 1, 3, 1, 4, 5, 6, 3, 4, 5, 2, 3, 4, 5, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 6, 2, 4, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 5, 2, 3, 5, 6, 4, 6, 3, 2, 4, 2, 3, 1, 2, 3, 4, 5, 6, 5, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 5, 2, 3, 4, 5, 6, 3, 6, 2, 3, 4, 6, 6, 6, 2, 5, 5, 4, 1, 4, 6, 4, 5, 4, 1, 3, 2, 3, 6, 1, 1, 2, 1, 2, 3, 4, 5, 6, 4, 6, 3, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 2, 3, 4, 5, 6, 1, 3, 5, 1, 2, 3, 4, 6, 3, 4, 1, 6, 2, 2, 3, 4, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 6, 1, 2, 4, 6, 1, 3, 4, 5, 6, 1, 3, 4, 6, 5, 4, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 4, 6, 2, 4, 6, 3, 2, 3, 4, 5, 6, 1, 3, 4, 6, 1, 3, 4, 5, 6, 1, 4, 5, 6, 1, 3, 2, 4, 6, 1, 6, 1, 2, 3, 5, 1, 3, 4, 5, 6, 2, 3, 4, 6, 4, 6, 1, 3, 5, 2, 3, 4, 5, 6, 1, 6, 1, 1, 2, 5, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 1, 2, 4, 1, 2, 2, 4, 5, 5, 2, 5, 2, 4, 5, 6, 4, 5, 1, 3, 4, 5, 6, 1, 2, 4, 5, 6, 6, 1, 2, 4, 6, 5, 2, 3, 4, 6, 5, 3, 2, 3, 4, 5, 6, 4, 6, 1], \"Freq\": [0.98286807756017, 0.002312630770729812, 0.002312630770729812, 0.011563153853649059, 0.0034160300660367442, 0.0034160300660367442, 0.9906487191506559, 0.9944884659643635, 0.012109679955821021, 0.012109679955821021, 0.9687743964656818, 0.98849984511016, 0.9883779201458786, 0.976739314642241, 0.31348331825784176, 0.005952214903629907, 0.003968143269086605, 0.11309208316896824, 0.5614922725757546, 0.0034873251064680244, 0.15344230468459308, 0.0034873251064680244, 0.006974650212936049, 0.0034873251064680244, 0.8299833753393898, 0.09566169415168285, 0.0045553187691277554, 0.0045553187691277554, 0.89284247874904, 0.03664844794215734, 0.03018107477589428, 0.00215579105542102, 0.9291459448864596, 0.07106465436629301, 0.030759328009291004, 0.0021213329661580005, 0.8952025117186762, 0.1685675408335016, 0.06684574895121614, 0.7614602706616795, 0.08741579260160581, 0.1174649713084078, 0.010926974075200727, 0.7840103898956521, 0.0014866262173237773, 0.008919757303942663, 0.0014866262173237773, 0.016352888390561548, 0.0014866262173237773, 0.9707669199124265, 0.9960241347999657, 0.9880164186828392, 0.6100318542770452, 0.09043598917853769, 0.10030173345256, 0.08385882632918949, 0.008221453561685245, 0.10687889630190818, 0.9591448613475335, 0.007612260804345504, 0.030449043217382017, 0.3766551218570274, 0.18885215023472124, 0.003147535837245354, 0.16314727423055086, 0.017836036411057007, 0.25022909906100566, 0.9248491012814672, 0.019761732933364685, 0.05138050562674818, 0.006175286238452816, 0.05804769064145646, 0.16302755669515434, 0.0703982631183621, 0.6533452840283079, 0.04940228990762253, 0.08238472594537863, 0.09751661438432573, 0.5178468487995228, 0.04875830719216286, 0.19671454970631225, 0.0571649118804668, 0.2972207410664647, 0.0030022297077420676, 0.6905128327806755, 0.0030022297077420676, 0.006004459415484135, 0.120465248998845, 0.1952639330177357, 0.3550969104475758, 0.1181031852929853, 0.0740113294502708, 0.13699969493986294, 0.3102620043508148, 0.03439861352585121, 0.5436329902320799, 0.05260964421600773, 0.0020234478544618358, 0.05733102254308535, 0.9854801697379505, 0.9953562946673262, 0.9949310247083822, 0.9190069890597355, 0.07776212984351609, 0.995457883516933, 0.981955538020272, 0.9660341374838873, 0.021955321306451985, 0.02172821018787805, 0.028970946917170736, 0.9415557748080489, 0.01743829262440385, 0.08137869891388463, 0.8951656880527309, 0.2055536400903615, 0.016121854124734235, 0.5110627757540752, 0.0411107280180723, 0.18701350784691714, 0.039498542605598874, 0.08603654567685166, 0.9052540892955696, 0.00374071937725442, 0.0006857297610631759, 0.42995256018661127, 0.0795446522833284, 0.30583547343417644, 0.18377557596493113, 0.9951897412986275, 0.9594374593679418, 0.025930742145079507, 0.9926277224834597, 0.9979471327315179, 0.8854114879317583, 0.08300732699360235, 0.04105888589062254, 0.15191787779530339, 0.02052944294531127, 0.7842247205108905, 0.9927139071213535, 0.4238745739759234, 0.036725594857119716, 0.10405585209517251, 0.1346605144761056, 0.15302331190466548, 0.14843261254752552, 0.050071245814658785, 0.18776717180497046, 0.7468960834019936, 0.012517811453664696, 0.9803961148704653, 0.02071259397613659, 0.9953543917508871, 0.182813272604345, 0.8066998378413953, 0.008705393933540237, 0.9873145075201005, 0.6401124138226478, 0.09750654540598867, 0.17785916152759043, 0.08396396965515691, 0.8409154265772573, 0.12546992079089236, 0.02936530061063438, 0.8524400830064645, 0.019248647035629843, 0.001374903359687846, 0.12649110909128183, 0.986243279033902, 0.9645010538045626, 0.013122463317068879, 0.01968369497560332, 0.9844756424800508, 0.007238791488823903, 0.9934702042393599, 0.34162845887236754, 0.012771157341023087, 0.6449434457216658, 0.9906597149299957, 0.9933406648606007, 0.9783204052736112, 0.10095330516621571, 0.3586811548258488, 0.26010322154589693, 0.12351933808572275, 0.03206752046456264, 0.1247070240288547, 0.06624602888328689, 0.2861264651767498, 0.0007047449881200734, 0.6462511541061072, 0.012826345893211355, 0.038479037679634065, 0.9427364231510346, 0.9699554561057206, 0.014476947106055533, 0.9883050045229204, 0.9975740961969656, 0.9810957864515104, 0.9967587958422319, 0.015689091106916988, 0.23533636660375482, 0.749938554910632, 0.1737216025965373, 0.0021990076278042693, 0.8224288527987967, 0.9849589384769322, 0.9676493335075707, 0.03247145414454935, 0.9925787144357215, 0.9941078954595931, 0.032228257132922995, 0.9614763377988693, 0.9908860116245207, 0.9864680149917253, 0.9963617482837679, 0.7767889955352657, 0.036830512719344496, 0.17745610673865983, 0.003348228429031318, 0.003348228429031318, 0.003348228429031318, 0.9941981094202765, 0.21409211640353484, 0.2265393324735078, 0.13318521194871064, 0.021160267318954026, 0.3497667715662401, 0.05476775070788101, 0.9902875745656472, 0.9867051822912999, 0.22024638066587687, 0.002117753660248816, 0.7772155933113155, 0.9898625786499793, 0.9918123809213366, 0.7004302059525234, 0.0018287994933486248, 0.2359151346419726, 0.031089591386926622, 0.0018287994933486248, 0.029260791893577998, 0.9775614698859384, 0.010742433735010312, 0.010742433735010312, 0.9985738682427865, 0.0005929773564387093, 0.0023512536365989313, 0.03291755091238504, 0.9146376646369843, 0.023512536365989312, 0.025863790002588243, 0.9971688204710633, 0.9842489717762025, 0.9829609782905335, 0.9761689482832921, 0.014569685795273018, 0.2172342939332739, 0.14289830630826655, 0.015155880972088876, 0.07938794794903697, 0.01659929820752591, 0.5297341254053922, 0.002422715033366896, 0.003634072550050344, 0.8200890387946943, 0.17322412488573305, 0.9911119690223044, 0.9878102830452614, 0.035372563354088435, 0.9594807809796487, 0.9870243991223158, 0.9188696140465645, 0.07754173958198857, 0.0019385434895497142, 0.0029734645922934377, 0.7047111083735448, 0.2051690568682472, 0.0862304731765097, 0.9947725031076451, 0.9833738714278001, 0.9927633281221252, 0.22309231192139772, 0.7388433556106505, 0.016791894445696603, 0.019190736509367545, 0.9528231340689749, 0.02575197659645878, 0.017167984397639186, 0.0006533879899947928, 0.006533879899947928, 0.0006533879899947928, 0.9918429688120954, 0.004478284842694302, 0.008956569685388605, 0.9852226653927465, 0.9336319747314838, 0.005245123453547662, 0.06294148144257194, 0.6525105526478107, 0.0012998218180235272, 0.11568414180409392, 0.11568414180409392, 0.09228734907967043, 0.022096970906399963, 0.2583196320329998, 0.029243731928264127, 0.6628579237073202, 0.048739553213773545, 0.1332434649485498, 0.8660825221655737, 0.5769127071869875, 0.041381924144214294, 0.001217115416006303, 0.1716132736568887, 0.18500154323295803, 0.024342308320126058, 0.9914794105430229, 0.2786058335099391, 0.057330442274608415, 0.3027449670992479, 0.1729971240567131, 0.11365508731632896, 0.07442899523370215, 0.27137848476367, 0.011902565121213595, 0.07558128851970633, 0.5570400476727962, 0.011902565121213595, 0.07201051898334225, 0.40108764024167703, 0.051775512939975524, 0.20296001072470404, 0.12149987036580923, 0.16361062089032263, 0.0593692548378386, 0.9843178127735968, 0.00841297275874869, 0.15751766662785116, 0.026252944437975195, 0.813841277577231, 0.9855589869715176, 0.8704609655069755, 0.12790446840102498, 0.9847757183253923, 0.9930511593183103, 0.003436163181032216, 0.9472324729613036, 0.046970205270808446, 0.3583339078882779, 0.02954007125960714, 0.2928320107474099, 0.0680705989895295, 0.0821984591571677, 0.16953432201165838, 0.9841192798725102, 0.9858222710763849, 0.005062054594319974, 0.000723150656331425, 0.5994918940987513, 0.00144630131266285, 0.35579012291506107, 0.03688068347290267, 0.20327312923339497, 0.000883796214058239, 0.1087069343291634, 0.6319142930516408, 0.054795365271610816, 0.029639401463922268, 0.9632805475774737, 0.21333273578675627, 0.044633444721342626, 0.16945579012848724, 0.3472330699507841, 0.18912476576840095, 0.03631195502753298, 0.11391981900405444, 0.007802727329044824, 0.12328309179890823, 0.5259038219776212, 0.18414436496545786, 0.043695273042651014, 0.3681259204144232, 0.021654465906730775, 0.6094185405179947, 0.7471451593873843, 0.0983085736036032, 0.0028088163886743774, 0.148867268599742, 0.0028088163886743774, 0.9879019143573045, 0.9904474749757519, 0.17422136859594567, 0.7954349354077519, 0.0035196236079989025, 0.02639717705999177, 0.9963803207196374, 0.9933875004452213, 0.9734755404697085, 0.9852738785554966, 0.9836025246906279, 0.9950290559773078, 0.11521188465297037, 0.7401490771645369, 0.13965076927632772, 0.9972596376191982, 0.9962376019288706, 0.9906840134217816, 0.008352132077954983, 0.9855515851986879, 0.9735468261252587, 0.02177261272484604, 0.006220746492813154, 0.9923697774336202, 0.9951379625331611, 0.9867805298609813, 0.6254042803852946, 0.06151517511986505, 0.0017087548644406957, 0.1930892996817986, 0.06493268484874644, 0.05297140079766157, 0.011648648633279414, 0.9784864851954708, 0.8599703698439676, 0.12474878821810641, 0.013271147682777278, 0.9974074767135583, 0.20841297478818713, 0.017643426437095207, 0.12791484166894024, 0.3197871041723506, 0.2183374021590532, 0.10861734400336737, 0.4071912782540088, 0.014451464990225782, 0.07820792818239834, 0.05610568760911186, 0.12071223697718006, 0.3238828330162366, 0.9916880710536787, 0.0923914092812724, 0.05475046475927254, 0.0034219040474545337, 0.7151779459179975, 0.13345425785072682, 0.008026832559799739, 0.08829515815779714, 0.8990052466975709, 0.0076617204151402185, 0.0012769534025233696, 0.4673649453235533, 0.09960236539682284, 0.4252254830402821, 0.9956322170911762, 0.975448840529961, 0.881659609294362, 0.11420461260289665, 0.9966798184186195, 0.08369189802689186, 0.8578419547756415, 0.05579459868459457, 0.9955146685035473, 0.029078336218672937, 0.9692778739557646, 0.9821744200804061, 0.1423051529972441, 0.004665742721221118, 0.07581831921984317, 0.2566158496671615, 0.1702996093245708, 0.3510971397718891, 0.0014548286643179546, 0.05071117058479727, 0.26436315157320545, 0.2516853589270061, 0.13509123311523863, 0.2965772148545316, 0.7613146162180834, 0.10429625739856319, 0.11919572274121507, 0.01441883742837279, 0.09449928326216203, 0.021338547833391425, 0.2103371143577155, 0.6736884387399293, 0.5837560607898884, 0.17683121549474723, 0.11291631832797111, 0.012782979433355221, 0.11078582175574525, 0.0022750573674528104, 0.9031977748787657, 0.011375286837264053, 0.08190206522830118, 0.9912096057568013, 0.9597941299648584, 0.03199313766549528, 0.9962267736456281, 0.054692871886528015, 0.630211046510675, 0.0012430198156029094, 0.25606208201419933, 0.012430198156029094, 0.04474871336170474, 0.3418932226224016, 0.0031804020709060615, 0.22898894910523643, 0.049296232099043956, 0.3768776454023683, 0.19066138006717911, 0.0859318896077427, 0.10875754778479935, 0.04430863057899233, 0.5397596815986339, 0.02953908705266155, 0.006221264311358456, 0.3670545943701489, 0.4429540189687221, 0.00870977003590184, 0.17668390644258017, 0.9946008116713204, 0.8931904102757525, 0.10606636122024561, 0.0005582440064223454, 0.3765056101329954, 0.5702628831838936, 0.05174199905336486, 0.9880370619041565, 0.36582710578289784, 0.0060971184297149645, 0.18748639171373516, 0.009145677644572447, 0.4313711289023337, 0.08433418416090825, 0.025300255248272476, 0.06325063812068119, 0.8222582955688555, 0.2725483727670588, 0.4195977273762626, 0.24719503576547192, 0.050706674003173725, 0.010141334800634744, 0.3839180781476587, 0.46827675899624743, 0.14289327572311963, 0.0034432114632077016, 0.9917432345519521, 0.9909040196556885, 0.3430380312035159, 0.39997081369317505, 0.2565578552698564, 0.9948790018162994, 0.9847716453555652, 0.9910035786716932, 0.6893726664517993, 0.10243690224384167, 0.20764236941319256, 0.7416503528105604, 0.04762892174012773, 0.10659806294219064, 0.0453608778477407, 0.058969141202062904, 0.6484203973828109, 0.06612506335562912, 0.0907985944584758, 0.193440483846318, 0.9872859572440924, 0.007002028065560939, 0.001385436483316419, 0.7412085185742842, 0.2563057494135375, 0.09890146658931179, 0.7558897803611686, 0.0023547968235550425, 0.13657821576619247, 0.004709593647110085, 0.016713836214662593, 0.9777594185577616, 0.9874538786925458, 0.9957767525385467, 0.0327897015592748, 0.9590987706087878, 0.02222215627254705, 0.40888767541486576, 0.00148147708483647, 0.28444360028860227, 0.16888838767135758, 0.1140737355324082, 0.5728089946455266, 0.18325277695903167, 0.13484638304532517, 0.05877919260950072, 0.05071146029054964, 0.9971020360086043, 0.0030434062020960077, 0.18260437212576047, 0.029419593286928076, 0.14101115403044837, 0.011159156074352028, 0.6320140213019376, 0.3825238285705819, 0.04533026658736626, 0.2043838335605812, 0.18688794119352756, 0.12406178223910766, 0.05566874844062523, 0.37014448755170126, 0.09694260388258842, 0.13660094183455643, 0.17500028493090639, 0.15737435695225394, 0.0642087376365196, 0.021511890405918502, 0.03764580821035738, 0.7529161642071476, 0.18285106845030727, 0.9887828248598186, 0.9787557029663969, 0.016495882634265116, 0.15165711630912504, 0.8474956499627575, 0.9937877678782142, 0.9824533893982116, 0.9820826532438958, 0.986786392875972, 0.010885881108501068, 0.9851722403193467, 0.30448371552582476, 0.5043011538396472, 0.10847232365607506, 0.0837330217696018, 0.8808464931811797, 0.09945041052045576, 0.9409966994541957, 0.02822990098362587, 0.014114950491812935, 0.014114950491812935, 0.004704983497270978, 0.2355104393940488, 0.06764661557063104, 0.03006516247583602, 0.6539172838494334, 0.01252715103159834, 0.9852570485332228, 0.9832097674559682, 0.9884021074696606, 0.8836622062453897, 0.10551190522333012, 0.9882792185299633, 0.12262046116673889, 0.3985164987919014, 0.18491956643693686, 0.2936957819880762, 0.9971822876717013, 0.9928268949522515, 0.7045455960272856, 0.0013120029721178502, 0.10758424371366372, 0.027552062414474856, 0.1587523596262599, 0.015131969170393257, 0.9785340063520972, 0.9932732862131167], \"Term\": [\"000\", \"000\", \"000\", \"000\", \"01\", \"01\", \"01\", \"05\", \"06\", \"06\", \"06\", \"07\", \"08\", \"09\", \"10\", \"10\", \"10\", \"10\", \"10\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"16\", \"16\", \"16\", \"16\", \"17\", \"17\", \"17\", \"17\", \"18\", \"18\", \"18\", \"18\", \"19\", \"19\", \"19\", \"2017\", \"2017\", \"2017\", \"2017\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2019\", \"40\", \"able\", \"able\", \"able\", \"able\", \"able\", \"able\", \"access\", \"access\", \"access\", \"account\", \"account\", \"account\", \"account\", \"account\", \"account\", \"accounts\", \"accounts\", \"accounts\", \"add\", \"add\", \"add\", \"add\", \"add\", \"add\", \"added\", \"added\", \"added\", \"added\", \"added\", \"added\", \"address\", \"address\", \"address\", \"address\", \"address\", \"advise\", \"advise\", \"advise\", \"advise\", \"advise\", \"advise\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"ai\", \"aid\", \"ap\", \"application\", \"application\", \"applied\", \"approval\", \"approve\", \"approve\", \"approved\", \"approved\", \"approved\", \"asap\", \"asap\", \"asap\", \"attached\", \"attached\", \"attached\", \"attached\", \"attached\", \"attached\", \"audit\", \"audit\", \"audit\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"ba\", \"bind\", \"bind\", \"binder\", \"blanket\", \"blocking\", \"blocking\", \"bua\", \"bua\", \"bua\", \"bua\", \"bureau\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"ca\", \"ca\", \"ca\", \"ca\", \"calculation\", \"calculation\", \"cat\", \"center\", \"center\", \"center\", \"cf\", \"change\", \"change\", \"change\", \"change\", \"class\", \"class\", \"class\", \"cna\", \"cna\", \"cna\", \"cna\", \"collision\", \"com\", \"com\", \"com\", \"commission\", \"commission\", \"compensation\", \"contact\", \"contact\", \"contact\", \"contractor\", \"contractors\", \"copies\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"correct\", \"coverage\", \"coverage\", \"coverage\", \"coverage\", \"covered\", \"covered\", \"covered\", \"credits\", \"credits\", \"crit\", \"cue\", \"customer\", \"damage\", \"data\", \"data\", \"data\", \"date\", \"date\", \"date\", \"debit\", \"decrease\", \"decrease\", \"deductibles\", \"department\", \"desk\", \"desk\", \"direction\", \"discretionary\", \"dmf\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"documents\", \"does\", \"does\", \"does\", \"does\", \"does\", \"does\", \"eb\", \"edit\", \"effective\", \"effective\", \"effective\", \"electronic\", \"eligible\", \"email\", \"email\", \"email\", \"email\", \"email\", \"email\", \"emails\", \"emails\", \"emails\", \"endorsement\", \"endorsement\", \"endorsements\", \"endorsements\", \"endorsements\", \"endorsements\", \"endorsements\", \"endt\", \"engine\", \"entities\", \"entity\", \"entity\", \"epc\", \"epc\", \"epc\", \"epc\", \"epc\", \"epc\", \"error\", \"error\", \"error\", \"error\", \"excel\", \"excluded\", \"exclusion\", \"exclusion\", \"expiration\", \"expiring\", \"expiring\", \"expiring\", \"exposure\", \"exposure\", \"exposure\", \"exposure\", \"fast\", \"feedback\", \"fein\", \"field\", \"field\", \"field\", \"field\", \"filing\", \"filing\", \"filing\", \"form\", \"form\", \"form\", \"form\", \"forms\", \"forms\", \"forms\", \"forward\", \"forward\", \"forward\", \"frc\", \"frc\", \"frc\", \"frc\", \"frc\", \"frc\", \"getting\", \"getting\", \"getting\", \"getting\", \"gl\", \"gl\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"guide\", \"hello\", \"hello\", \"hello\", \"hello\", \"hello\", \"hello\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hi\", \"hi\", \"hi\", \"hi\", \"hi\", \"hi\", \"higher\", \"higher\", \"hired\", \"hired\", \"hired\", \"history\", \"ies\", \"ies\", \"inception\", \"increase\", \"increase\", \"increased\", \"increased\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inland\", \"inspection\", \"insured\", \"insured\", \"insured\", \"insured\", \"insured\", \"insured\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"job\", \"job\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"liability\", \"liability\", \"liability\", \"limit\", \"limit\", \"limit\", \"limit\", \"limit\", \"linked\", \"loc\", \"location\", \"location\", \"location\", \"location\", \"loss\", \"losses\", \"lower\", \"manuscript\", \"marine\", \"market\", \"message\", \"message\", \"message\", \"middle\", \"mike\", \"mm\", \"mo\", \"mo\", \"mod\", \"mod\", \"mod\", \"model\", \"modeling\", \"mods\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"mvr\", \"mvr\", \"named\", \"named\", \"named\", \"ncci\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"noc\", \"non\", \"non\", \"non\", \"non\", \"non\", \"notice\", \"notice\", \"notice\", \"number\", \"number\", \"number\", \"number\", \"number\", \"ny\", \"occurred\", \"open\", \"open\", \"option\", \"payroll\", \"payroll\", \"payroll\", \"physical\", \"plus\", \"plus\", \"pm\", \"policies\", \"policies\", \"policies\", \"policies\", \"policies\", \"policies\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"premium\", \"premium\", \"premium\", \"premium\", \"prior\", \"prior\", \"prior\", \"prior\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processed\", \"processed\", \"processed\", \"processed\", \"product\", \"progress\", \"progress\", \"proof\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"quote\", \"quote\", \"quote\", \"quote\", \"quote\", \"rapid\", \"rate\", \"rate\", \"rate\", \"rating\", \"rating\", \"rating\", \"registered\", \"renewal\", \"renewal\", \"renewal\", \"renewal\", \"renewal\", \"report\", \"report\", \"report\", \"report\", \"request\", \"request\", \"request\", \"request\", \"request\", \"resource\", \"resource\", \"resource\", \"resource\", \"right\", \"rp\", \"rst\", \"rst\", \"rst\", \"run\", \"runs\", \"save\", \"schedule\", \"schedule\", \"schedule\", \"send\", \"send\", \"send\", \"send\", \"send\", \"showing\", \"showing\", \"showing\", \"showing\", \"small\", \"small\", \"state\", \"state\", \"state\", \"states\", \"states\", \"states\", \"states\", \"states\", \"status\", \"status\", \"stop\", \"submission\", \"symbol\", \"symbol\", \"tap\", \"tap\", \"tap\", \"tap\", \"tap\", \"tap\", \"team\", \"team\", \"team\", \"team\", \"team\", \"technical\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"thanks\", \"ticket\", \"ticket\", \"ticket\", \"ticket\", \"tool\", \"total\", \"total\", \"track\", \"track\", \"trailers\", \"tria\", \"trucks\", \"uim\", \"um\", \"um\", \"umbrella\", \"umbrella\", \"umbrella\", \"umbrella\", \"underlying\", \"underlying\", \"underwriting\", \"underwriting\", \"underwriting\", \"underwriting\", \"underwriting\", \"use\", \"use\", \"use\", \"use\", \"use\", \"utilized\", \"uts\", \"vs\", \"wait\", \"wait\", \"waiver\", \"wc\", \"wc\", \"wc\", \"wc\", \"wording\", \"workers\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"years\", \"zone\"]}, \"R\": 20, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 5, 3, 6, 4, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el215611125812072888455132326\", ldavis_el215611125812072888455132326_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el215611125812072888455132326\", ldavis_el215611125812072888455132326_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el215611125812072888455132326\", ldavis_el215611125812072888455132326_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.117051  0.182855       1        1  20.684472\n",
       "4      0.205982 -0.172436       2        1  16.799726\n",
       "2     -0.111317 -0.038096       3        1  16.278699\n",
       "5      0.087870  0.084565       4        1  15.722306\n",
       "3     -0.205972 -0.153677       5        1  15.510227\n",
       "1      0.140488  0.096789       6        1  15.004571, topic_info=     Category         Freq         Term        Total  loglift  logprob\n",
       "term                                                                  \n",
       "330   Default  1686.000000  endorsement  1686.000000  20.0000  20.0000\n",
       "397   Default  1530.000000         form  1530.000000  19.0000  19.0000\n",
       "730   Default  1791.000000         rate  1791.000000  18.0000  18.0000\n",
       "682   Default  2080.000000      premium  2080.000000  17.0000  17.0000\n",
       "676   Default  4811.000000       policy  4811.000000  16.0000  16.0000\n",
       "253   Default  1418.000000     coverage  1418.000000  15.0000  15.0000\n",
       "416   Default   975.000000           gl   975.000000  14.0000  14.0000\n",
       "20    Default   942.000000           18   942.000000  13.0000  13.0000\n",
       "486   Default  1382.000000      insured  1382.000000  12.0000  12.0000\n",
       "27    Default   672.000000         2018   672.000000  11.0000  11.0000\n",
       "344   Default   825.000000        error   825.000000  10.0000  10.0000\n",
       "439   Default  1680.000000         help  1680.000000   9.0000   9.0000\n",
       "802   Default  1387.000000          rst  1387.000000   8.0000   8.0000\n",
       "767   Default  1312.000000      renewal  1312.000000   7.0000   7.0000\n",
       "494   Default  1131.000000        issue  1131.000000   6.0000   6.0000\n",
       "73    Default  1482.000000        agent  1482.000000   5.0000   5.0000\n",
       "550   Default   474.000000         loss   474.000000   4.0000   4.0000\n",
       "734   Default   908.000000       rating   908.000000   3.0000   3.0000\n",
       "856   Default   721.000000        state   721.000000   2.0000   2.0000\n",
       "122   Default  1458.000000         auto  1458.000000   1.0000   1.0000\n",
       "177    Topic1   207.118992          cat   207.966129   1.5717  -5.2346\n",
       "299    Topic1   225.881170          dmf   226.825247   1.5716  -5.1479\n",
       "591    Topic1   148.289794        model   149.137956   1.5701  -5.5687\n",
       "302    Topic1   147.008019    documents   147.857855   1.5700  -5.5774\n",
       "799    Topic1   173.365005        right   174.440313   1.5696  -5.4125\n",
       "866    Topic1   146.697855   submission   147.623450   1.5695  -5.5795\n",
       "729    Topic1   104.696242        rapid   105.569992   1.5675  -5.9168\n",
       "906    Topic1   129.378268         tool   130.463431   1.5674  -5.7051\n",
       "293    Topic1   115.052750    direction   116.057749   1.5671  -5.8225\n",
       "804    Topic1   127.529655          run   128.658862   1.5670  -5.7195\n",
       "...       ...          ...          ...          ...      ...      ...\n",
       "242    Topic6    44.061875       copies    44.975041   1.8763  -6.4612\n",
       "995    Topic6   194.182711        years   198.255757   1.8761  -4.9780\n",
       "861    Topic6   117.171953       status   119.661338   1.8758  -5.4832\n",
       "948    Topic6    37.713313     utilized    38.568615   1.8744  -6.6168\n",
       "27     Topic6   652.578183         2018   672.664042   1.8665  -3.7659\n",
       "7      Topic6    80.447731           06    82.578566   1.8707  -5.8592\n",
       "19     Topic6   431.017627           17   463.866847   1.8234  -4.1807\n",
       "20     Topic6   843.549973           18   942.803432   1.7856  -3.5092\n",
       "18     Topic6   196.493592           16   219.523605   1.7860  -4.9662\n",
       "270    Topic6   374.311897         date   454.750583   1.7022  -4.3217\n",
       "14     Topic6   237.705071           12   286.752732   1.7092  -4.7758\n",
       "316    Topic6   366.822438    effective   472.198452   1.6443  -4.3420\n",
       "26     Topic6   287.401380         2017   366.066577   1.6549  -4.5860\n",
       "771    Topic6   195.156366       report   237.151758   1.7019  -4.9730\n",
       "21     Topic6   262.151619           19   344.075732   1.6249  -4.6779\n",
       "885    Topic6   623.466865         term   985.737624   1.4387  -3.8115\n",
       "269    Topic6   238.568891         data   318.692776   1.6072  -4.7722\n",
       "155    Topic6   190.850343          bua   243.552639   1.6530  -4.9953\n",
       "340    Topic6   733.524577          epc  1385.600747   1.2608  -3.6490\n",
       "676    Topic6  1426.733664       policy  4811.563156   0.6812  -2.9837\n",
       "767    Topic6   566.034813      renewal  1312.095229   1.0561  -3.9082\n",
       "11     Topic6   282.896392           10   504.014060   1.3193  -4.6018\n",
       "691    Topic6   220.685903        prior   328.044816   1.5004  -4.8501\n",
       "622    Topic6   332.580163       number   783.113932   1.0404  -4.4400\n",
       "51     Topic6   477.409194      account  1906.253117   0.5123  -4.0785\n",
       "612    Topic6   380.731867          new  1176.351326   0.7687  -4.3047\n",
       "802    Topic6   356.183399          rst  1387.601247   0.5369  -4.3714\n",
       "675    Topic6   300.637272     policies   857.312595   0.8489  -4.5409\n",
       "971    Topic6   297.088024           wc  1011.250478   0.6719  -4.5528\n",
       "122    Topic6   267.868876         auto  1458.300422   0.2023  -4.6563\n",
       "\n",
       "[293 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "1         2  0.982868           000\n",
       "1         4  0.002313           000\n",
       "1         5  0.002313           000\n",
       "1         6  0.011563           000\n",
       "2         3  0.003416            01\n",
       "2         5  0.003416            01\n",
       "2         6  0.990649            01\n",
       "6         6  0.994488            05\n",
       "7         2  0.012110            06\n",
       "7         3  0.012110            06\n",
       "7         6  0.968774            06\n",
       "8         6  0.988500            07\n",
       "9         6  0.988378            08\n",
       "10        6  0.976739            09\n",
       "11        2  0.313483            10\n",
       "11        3  0.005952            10\n",
       "11        4  0.003968            10\n",
       "11        5  0.113092            10\n",
       "11        6  0.561492            10\n",
       "14        1  0.003487            12\n",
       "14        2  0.153442            12\n",
       "14        3  0.003487            12\n",
       "14        4  0.006975            12\n",
       "14        5  0.003487            12\n",
       "14        6  0.829983            12\n",
       "18        2  0.095662            16\n",
       "18        3  0.004555            16\n",
       "18        5  0.004555            16\n",
       "18        6  0.892842            16\n",
       "19        2  0.036648            17\n",
       "...     ...       ...           ...\n",
       "934       1  0.940997  underwriting\n",
       "934       3  0.028230  underwriting\n",
       "934       4  0.014115  underwriting\n",
       "934       5  0.014115  underwriting\n",
       "934       6  0.004705  underwriting\n",
       "942       1  0.235510           use\n",
       "942       2  0.067647           use\n",
       "942       4  0.030065           use\n",
       "942       5  0.653917           use\n",
       "942       6  0.012527           use\n",
       "948       6  0.985257      utilized\n",
       "949       1  0.983210           uts\n",
       "962       2  0.988402            vs\n",
       "963       4  0.883662          wait\n",
       "963       6  0.105512          wait\n",
       "965       5  0.988279        waiver\n",
       "971       2  0.122620            wc\n",
       "971       3  0.398516            wc\n",
       "971       4  0.184920            wc\n",
       "971       6  0.293696            wc\n",
       "980       5  0.997182       wording\n",
       "983       3  0.992827       workers\n",
       "994       2  0.704546          year\n",
       "994       3  0.001312          year\n",
       "994       4  0.107584          year\n",
       "994       5  0.027552          year\n",
       "994       6  0.158752          year\n",
       "995       4  0.015132         years\n",
       "995       6  0.978534         years\n",
       "999       1  0.993273          zone\n",
       "\n",
       "[664 rows x 3 columns], R=20, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 5, 3, 6, 4, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 6\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                      learning_method='online',\n",
    "                                      learning_offset=50.,\n",
    "                                      random_state=123)\n",
    "\n",
    "lda_model.fit(tf)\n",
    "pyLDAvis.sklearn.prepare(lda_model,tf, tf_vectorizer, R=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        _top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        top_words[str(topic_idx)] = _top_words\n",
    "    return(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "token-topic matrix (1000, 6)\n",
      "0 account cna thanks hi frc team thank new good agent help email need able morning\n",
      "1 policy 18 epc 2018 term renewal account loss 17 new date effective rst number policies\n",
      "2 endorsement policy insured agent attached state location advise coverage wc processed endorsements number request named\n",
      "3 form coverage policy add insured auto forms question need does use know thanks hi provide\n",
      "4 rate premium change showing auto year property renewal rst expiring 000 account rating class mod\n",
      "5 policy help gl issue error need rst rating know quote let account field center thanks\n",
      "total words: 67\n"
     ]
    }
   ],
   "source": [
    "## get the token to topic matrix\n",
    "word_topic = np.zeros((max_features,n_topics),)\n",
    "print(n_topics)\n",
    "lda_model.components_\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    word_topic[:,topic_idx] = topic\n",
    "\n",
    "print(\"token-topic matrix\",word_topic.shape)\n",
    "\n",
    "## create a matrix of the top words used to define each topic\n",
    "top_words = 15\n",
    "tf_feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "top_words = get_top_words(lda_model,tf_feature_names,top_words)\n",
    "all_top_words = np.array(list(set().union(*[v for v in top_words.values()])))\n",
    "\n",
    "for key,vals in top_words.items():\n",
    "    print(key,\" \".join(vals))\n",
    "print(\"total words: %s\"%len(all_top_words))\n",
    "\n",
    "top_word_inds = [np.where(tf_feature_names == tw)[0][0] for tw in all_top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
